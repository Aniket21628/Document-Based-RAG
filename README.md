# Document Based RAG Chatbot

A production-ready Retrieval-Augmented Generation (RAG) chatbot system that enables intelligent Q&A over multiple document formats. Built with FastAPI, React, ChromaDB, and Google Gemini AI.

## üåü Features

- **Multi-Format Document Support**: Process PDF, DOCX, PPTX, CSV, TXT, and Markdown files
- **Intelligent Vector Search**: ChromaDB with Cohere embeddings for semantic document retrieval
- **Conversational AI**: Google Gemini 2.0 Flash for contextual responses with conversation history
- **Modern UI**: React with TypeScript and Tailwind CSS for a sleek, responsive interface
- **Session Management**: Per-session conversation history tracking
- **Real-time Processing**: Instant document indexing and query responses
- **Source Attribution**: Transparent source citations for all responses

## üèóÔ∏è Architecture

### Backend Stack
- **FastAPI**: High-performance async web framework
- **ChromaDB**: Vector database for document embeddings
- **Cohere Embeddings**: `embed-english-v3.0` for semantic search
- **Google Gemini AI**: `gemini-2.0-flash-exp` for response generation
- **Python 3.8+**: Core backend runtime

### Frontend Stack
- **React 19**: Modern UI framework
- **TypeScript**: Type-safe development
- **Tailwind CSS 4**: Utility-first styling
- **Vite 7**: Lightning-fast build tool
- **Lucide React**: Beautiful icon system

### Key Components

```
backend/
‚îú‚îÄ‚îÄ main.py                 # FastAPI application & endpoints
‚îú‚îÄ‚îÄ config.py              # Configuration management
‚îú‚îÄ‚îÄ document_parser.py     # Multi-format document processors
‚îú‚îÄ‚îÄ vector_store.py        # ChromaDB integration
‚îî‚îÄ‚îÄ requirements.txt       # Python dependencies

frontend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ RAGChatbot.tsx # Main chat interface
‚îÇ   ‚îú‚îÄ‚îÄ App.tsx            # Root component
‚îÇ   ‚îî‚îÄ‚îÄ main.tsx           # Entry point
‚îú‚îÄ‚îÄ package.json           # Node dependencies
‚îî‚îÄ‚îÄ vite.config.ts         # Vite configuration
```

## üöÄ Getting Started

### Prerequisites

- **Python**: 3.8 or higher
- **Node.js**: 16.x or higher
- **API Keys**:
  - Google Gemini API Key
  - Cohere API Key

### Backend Setup

1. **Clone the Repository**
   ```bash
   git clone https://github.com/Aniket21628/Document-Based-RAG.git
   cd agentic-rag-chatbot
   ```

2. **Create Virtual Environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Windows: venv\Scripts\activate
   ```

3. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure Environment Variables**
   ```bash
   # Create .env file in backend directory
   echo "GEMINI_API_KEY=your-gemini-api-key" > .env
   echo "COHERE_API_KEY=your-cohere-api-key" >> .env
   ```

5. **Run Backend Server**
   ```bash
   uvicorn main:app --reload
   ```
   Server starts at `http://0.0.0.0:8000`

### Frontend Setup

1. **Navigate to Frontend Directory**
   ```bash
   cd frontend
   ```

2. **Install Dependencies**
   ```bash
   npm install
   ```

3. **Configure API Endpoint** (optional)
   ```bash
   # Create .env file in frontend directory
   echo "VITE_API_BASE_URL=http://localhost:8000" > .env
   ```

4. **Run Development Server**
   ```bash
   npm run dev
   ```
   Application starts at `http://localhost:5173`

## üìñ Usage

### Uploading Documents

1. Click the upload area or drag-and-drop files
2. Supported formats: PDF, DOCX, PPTX, CSV, TXT, MD
3. Files are automatically processed and indexed
4. Maximum file size: 50MB per file

### Asking Questions

1. Type your question in the input field
2. Press Enter or click Send
3. Receive AI-generated responses with source citations
4. View confidence scores and relevant document sections


## üîå API Endpoints

### Core Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/health` | Detailed health status |
| POST | `/upload` | Upload and process documents |
| POST | `/ask` | Ask questions with context |
| GET | `/conversation/{session_id}` | Get conversation history |
| DELETE | `/conversation/{session_id}` | Clear conversation history |

## ‚öôÔ∏è Configuration

### Backend Configuration (`config.py`)

```python
GEMINI_API_KEY: str          # Required: Google Gemini API key
COHERE_API_KEY: str          # Required: Cohere API key
CHROMA_PERSIST_DIRECTORY: str # Vector DB storage path
UPLOAD_DIRECTORY: str         # Uploaded files directory
EMBEDDING_MODEL: str          # "embed-english-v3.0"
MAX_FILE_SIZE: int           # 50MB default
HOST: str                     # "0.0.0.0"
PORT: int                     # 8000
```

### Environment Variables

**Backend (.env)**
```env
GEMINI_API_KEY=your-gemini-api-key
COHERE_API_KEY=your-cohere-api-key
CHROMA_PERSIST_DIRECTORY=/tmp/chroma_db  # Optional
UPLOAD_DIR=/tmp/uploads                   # Optional
PORT=8000                                  # Optional
```

**Frontend (.env)**
```env
VITE_API_BASE_URL=http://localhost:8000
```


## üìù Document Processing

### Supported Formats

| Format | Extension | Features |
|--------|-----------|----------|
| PDF | `.pdf` | Text extraction, page numbers |
| Word | `.docx` | Paragraphs, tables |
| PowerPoint | `.pptx` | Slides, shapes, tables |
| CSV | `.csv` | Headers, statistics, preview |
| Text | `.txt` | Plain text |
| Markdown | `.md` | Formatted text |

### Processing Pipeline

1. **File Upload**: Validated and saved temporarily
2. **Text Extraction**: Format-specific parsing
3. **Chunking**: Split into 1000-word chunks with 200-word overlap
4. **Embedding**: Generated using Cohere's `embed-english-v3.0`
5. **Storage**: Stored in ChromaDB with metadata
6. **Indexing**: Ready for semantic search

## üöÄ Deployment

### Render.com (Recommended)

1. Create new Web Service
2. Connect GitHub repository
3. Configure build:
   ```bash
   pip install -r requirements.txt
   ```
4. Start command:
   ```bash
   python main.py
   ```
5. Add environment variables in dashboard
6. Deploy frontend as separate Static Site


Built with ‚ù§Ô∏è using FastAPI, React, and AI